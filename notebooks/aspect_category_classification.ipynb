{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "aspect_category_classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlWJDuL9LEVu",
        "colab_type": "code",
        "outputId": "2d08b680-57eb-4ea8-ca07-082cef618111",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrZx8n-HL26v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVCv-zb1MaU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('ir_project/New folder/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFtdKN4mqcZB",
        "colab_type": "code",
        "outputId": "e9e531f6-0b00-4ea8-bc9d-1b36063c36e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'test_data.npy' in os.listdir('.')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEWKIYSZMvCr",
        "colab_type": "code",
        "outputId": "8bff1137-7b5a-4f9c-cf3a-a401ecfe3941",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.9.1)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.90)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l782EpUrM70F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import transformers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsEFfM2DMzRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUjEuM4zNSk-",
        "colab_type": "code",
        "outputId": "dac4910d-6117-44a1-ae93-ebcb32037f00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import transformers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils import data\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "all_labels=[]\n",
        "train_data=np.load('train_data.npy',allow_pickle=True)\n",
        "for row in train_data:\n",
        "    if row[-1]=='':\n",
        "        continue\n",
        "    else:\n",
        "        all_labels.extend(row[-1]['category'])\n",
        "\n",
        "all_labels=list(set(all_labels))\n",
        "\n",
        "print(\"number of classes = \",len(all_labels))\n",
        "\n",
        "labels_mapping={}\n",
        "\n",
        "idx=0\n",
        "labels_mapping['NIL']=0\n",
        "for label in all_labels:\n",
        "    idx+=1\n",
        "    labels_mapping[label]=idx\n",
        "\n",
        "x_train=[]\n",
        "y_train=[]\n",
        "for row in train_data:\n",
        "    sentence = row[2]\n",
        "    \n",
        "    x_train.append(sentence)\n",
        "    \n",
        "    if row[-1]=='':\n",
        "        y_train.append([0])\n",
        "        continue\n",
        "    a=[]\n",
        "    labels = row[-1]['category']\n",
        "    for i in labels:\n",
        "        \n",
        "        a.append(labels_mapping[i])\n",
        "    y_train.append(a)\n",
        "\n",
        "tokenized_sentences = []\n",
        "labels=[]\n",
        "\n",
        "for (s,l) in zip(x_train,y_train) :\n",
        "    if isinstance(s,float):\n",
        "        continue\n",
        "    a=tokenizer.encode(s,add_sepcial_tokens=True)\n",
        "    tokenized_sentences.append(a)\n",
        "    labels.append(l)\n",
        "\n",
        "max_len = 0\n",
        "for i in tokenized_sentences:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized_sentences])\n",
        "input_ids = torch.LongTensor(np.array(padded))\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "'''\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids)\n",
        "'''\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \n",
        "    def __init__(self):\n",
        "        super(Model, self).__init__()\n",
        "        self.model_class, self.tokenizer_class, self.pretrained_weights = (transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "        self.model = model_class.from_pretrained(pretrained_weights)\n",
        "        \n",
        "        self.drop_out=nn.Dropout(0.1)\n",
        "        self.conv1=nn.Conv1d(768*2,768,1)\n",
        "        \n",
        "        self.output=nn.Linear(768,82)\n",
        "    \n",
        "    def forward(self,x,attention_masks):\n",
        "        \n",
        "        x = self.model(x,attention_masks)\n",
        "        #print(x[0].shape)\n",
        "        x = torch.cat((x[0][:,-1,:],x[0][:,-2,:]),dim=-1)\n",
        "        #print(x.shape)\n",
        "        x = x.unsqueeze(2)\n",
        "        #print(x.shape)\n",
        "        x=self.drop_out(x)\n",
        "        \n",
        "        x=self.conv1(x)\n",
        "        #print(x.shape)\n",
        "        x=F.relu(x)\n",
        "        #print(x.shape)\n",
        "        \n",
        "        x=x.squeeze(2)\n",
        "        #print(x.shape)\n",
        "        x=self.output(x)\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "    \n",
        "class TargetLoader(data.Dataset):\n",
        "\n",
        "  def __init__(self,sentences,new_y_train,attention_mask):\n",
        "\n",
        "    self.sentences=sentences\n",
        "    self.new_y_train=new_y_train\n",
        "    self.attention_mask=attention_mask\n",
        "    \n",
        "    self.length=len(self.sentences)\n",
        "\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "    image=self.sentences[idx]\n",
        "    label=self.new_y_train[idx]\n",
        "    mask = self.attention_mask[idx]\n",
        "    return image,label,mask\n",
        "  \n",
        "  def __len__(self):\n",
        "\n",
        "    return self.length\n",
        "\n",
        "\n",
        "new_y_train=[]\n",
        "#https://discuss.pytorch.org/t/what-kind-of-loss-is-better-to-use-in-multilabel-classification/32203/4\n",
        "\n",
        "for label in y_train:\n",
        "    a=torch.LongTensor(label)\n",
        "    a=a.unsqueeze(0)\n",
        "    target = torch.zeros(a.size(0),82).scatter_(1, a, 1.)\n",
        "    new_y_train.append(target.numpy()[0])\n",
        "\n",
        "new_y_train=torch.FloatTensor(np.array(new_y_train))\n",
        "\n",
        "\n",
        "train_target=TargetLoader(input_ids,new_y_train,attention_mask)\n",
        "\n",
        "train_loader=torch.utils.data.DataLoader(train_target,batch_size=16,shuffle=True)\n",
        "\n",
        "net = Model()\n",
        "net = net.cuda()\n",
        "optimizer = optim.Adadelta(net.parameters(), lr=1.0, rho=0.95, eps=1e-06, weight_decay=0)\n",
        "criterion=nn.BCEWithLogitsLoss()\n",
        "\n",
        "for epoch in range(15):\n",
        "    \n",
        "    total_loss=0\n",
        "    for sentences,labels,mask in train_loader:\n",
        "        #break\n",
        "        sentences = sentences.cuda()\n",
        "        labels = labels.cuda()\n",
        "        mask=mask.cuda()\n",
        "        preds = net(sentences,mask)\n",
        "        \n",
        "        loss = criterion(preds,labels)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        #break\n",
        "        total_loss+=loss.item()\n",
        "        \n",
        "        print(\"loss :\",loss.item())\n",
        "    print(\"epoch done !\",epoch, \" loss : \",total_loss)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of classes =  81\n",
            "loss : 0.686030924320221\n",
            "loss : 0.6333944201469421\n",
            "loss : 0.5419178009033203\n",
            "loss : 0.36799490451812744\n",
            "loss : 0.2833956778049469\n",
            "loss : 0.08387395739555359\n",
            "loss : 0.06588933616876602\n",
            "loss : 0.06877664476633072\n",
            "loss : 0.06592719256877899\n",
            "loss : 0.07223622500896454\n",
            "loss : 0.06904394179582596\n",
            "loss : 0.06102750077843666\n",
            "loss : 0.07113512605428696\n",
            "loss : 0.062367234379053116\n",
            "loss : 0.06849164515733719\n",
            "loss : 0.06352962553501129\n",
            "loss : 0.0663682296872139\n",
            "loss : 0.0614696741104126\n",
            "loss : 0.0674188956618309\n",
            "loss : 0.06610897928476334\n",
            "loss : 0.07582955062389374\n",
            "loss : 0.05743352696299553\n",
            "loss : 0.06381037831306458\n",
            "loss : 0.0526198111474514\n",
            "loss : 0.05293043330311775\n",
            "loss : 0.05869261547923088\n",
            "loss : 0.054453760385513306\n",
            "loss : 0.0727647989988327\n",
            "loss : 0.07284417748451233\n",
            "loss : 0.05234520509839058\n",
            "loss : 0.07233911007642746\n",
            "loss : 0.07580987364053726\n",
            "loss : 0.07280539721250534\n",
            "loss : 0.06659400463104248\n",
            "loss : 0.05474301055073738\n",
            "loss : 0.04686954617500305\n",
            "loss : 0.07661721110343933\n",
            "loss : 0.07683943212032318\n",
            "loss : 0.07074912637472153\n",
            "loss : 0.08077642321586609\n",
            "loss : 0.07420476526021957\n",
            "loss : 0.04711711406707764\n",
            "loss : 0.058985766023397446\n",
            "loss : 0.06631586700677872\n",
            "loss : 0.058635931462049484\n",
            "loss : 0.07248920202255249\n",
            "loss : 0.06916221231222153\n",
            "loss : 0.053790271282196045\n",
            "loss : 0.07034233957529068\n",
            "loss : 0.07956638187170029\n",
            "loss : 0.05846527963876724\n",
            "loss : 0.06407197564840317\n",
            "loss : 0.066429004073143\n",
            "loss : 0.07087931782007217\n",
            "loss : 0.06398869305849075\n",
            "loss : 0.06516687572002411\n",
            "loss : 0.05041704326868057\n",
            "loss : 0.060809992253780365\n",
            "loss : 0.05163003131747246\n",
            "loss : 0.057745255529880524\n",
            "loss : 0.06444425135850906\n",
            "loss : 0.06740202009677887\n",
            "loss : 0.05987027660012245\n",
            "loss : 0.06336233019828796\n",
            "loss : 0.08843235671520233\n",
            "loss : 0.0521777980029583\n",
            "loss : 0.07022886723279953\n",
            "loss : 0.06406223028898239\n",
            "loss : 0.06941629946231842\n",
            "loss : 0.06879472732543945\n",
            "loss : 0.05883103981614113\n",
            "loss : 0.06517338752746582\n",
            "loss : 0.06265341490507126\n",
            "loss : 0.057983942329883575\n",
            "loss : 0.06712852418422699\n",
            "loss : 0.05502421408891678\n",
            "loss : 0.05549280717968941\n",
            "loss : 0.07227075099945068\n",
            "loss : 0.06627070903778076\n",
            "loss : 0.07054004073143005\n",
            "loss : 0.07057495415210724\n",
            "loss : 0.05896620824933052\n",
            "loss : 0.07027037441730499\n",
            "loss : 0.061678290367126465\n",
            "loss : 0.06875959783792496\n",
            "loss : 0.05501275882124901\n",
            "loss : 0.0666370689868927\n",
            "loss : 0.04446028918027878\n",
            "loss : 0.0882442370057106\n",
            "loss : 0.06129131838679314\n",
            "loss : 0.04459352046251297\n",
            "loss : 0.07606450468301773\n",
            "loss : 0.06503993272781372\n",
            "loss : 0.07898262143135071\n",
            "loss : 0.058876123279333115\n",
            "loss : 0.07306516915559769\n",
            "loss : 0.06124863773584366\n",
            "loss : 0.06328604370355606\n",
            "loss : 0.048187196254730225\n",
            "loss : 0.06847728788852692\n",
            "loss : 0.061007894575595856\n",
            "loss : 0.05657355114817619\n",
            "loss : 0.06622590869665146\n",
            "loss : 0.06547648459672928\n",
            "loss : 0.059110622853040695\n",
            "loss : 0.06698601692914963\n",
            "loss : 0.08231554925441742\n",
            "loss : 0.07926066964864731\n",
            "loss : 0.05220114439725876\n",
            "epoch done ! 0  loss :  9.26691261306405\n",
            "loss : 0.04186907038092613\n",
            "loss : 0.0813017189502716\n",
            "loss : 0.06632303446531296\n",
            "loss : 0.058327242732048035\n",
            "loss : 0.0669512003660202\n",
            "loss : 0.041152264922857285\n",
            "loss : 0.08227898925542831\n",
            "loss : 0.0696202740073204\n",
            "loss : 0.06496581435203552\n",
            "loss : 0.04556700214743614\n",
            "loss : 0.06234661862254143\n",
            "loss : 0.056551337242126465\n",
            "loss : 0.07833978533744812\n",
            "loss : 0.06112086772918701\n",
            "loss : 0.05365050584077835\n",
            "loss : 0.05659022927284241\n",
            "loss : 0.05963236466050148\n",
            "loss : 0.056479405611753464\n",
            "loss : 0.05681471526622772\n",
            "loss : 0.07194375991821289\n",
            "loss : 0.06105518713593483\n",
            "loss : 0.062019769102334976\n",
            "loss : 0.05609526485204697\n",
            "loss : 0.06407134234905243\n",
            "loss : 0.05569981038570404\n",
            "loss : 0.05218423157930374\n",
            "loss : 0.06208318471908569\n",
            "loss : 0.06630903482437134\n",
            "loss : 0.05036865547299385\n",
            "loss : 0.054638173431158066\n",
            "loss : 0.051142800599336624\n",
            "loss : 0.060944706201553345\n",
            "loss : 0.06858699768781662\n",
            "loss : 0.054705385118722916\n",
            "loss : 0.051600709557533264\n",
            "loss : 0.06254042685031891\n",
            "loss : 0.043248772621154785\n",
            "loss : 0.04954470321536064\n",
            "loss : 0.06423207372426987\n",
            "loss : 0.06763481348752975\n",
            "loss : 0.04606667533516884\n",
            "loss : 0.059262920171022415\n",
            "loss : 0.05619675666093826\n",
            "loss : 0.07968363165855408\n",
            "loss : 0.05398391932249069\n",
            "loss : 0.06849389523267746\n",
            "loss : 0.05194134637713432\n",
            "loss : 0.0585038922727108\n",
            "loss : 0.05294065177440643\n",
            "loss : 0.05759380757808685\n",
            "loss : 0.07642936706542969\n",
            "loss : 0.06459514051675797\n",
            "loss : 0.04460718855261803\n",
            "loss : 0.04995439574122429\n",
            "loss : 0.06240736320614815\n",
            "loss : 0.0691259428858757\n",
            "loss : 0.06272552162408829\n",
            "loss : 0.0673595517873764\n",
            "loss : 0.052663568407297134\n",
            "loss : 0.06535453349351883\n",
            "loss : 0.05836266651749611\n",
            "loss : 0.062046896666288376\n",
            "loss : 0.04577222466468811\n",
            "loss : 0.0662904679775238\n",
            "loss : 0.0586882010102272\n",
            "loss : 0.06463760137557983\n",
            "loss : 0.05985462665557861\n",
            "loss : 0.06785353273153305\n",
            "loss : 0.0546768493950367\n",
            "loss : 0.06809740513563156\n",
            "loss : 0.05471813306212425\n",
            "loss : 0.05738728120923042\n",
            "loss : 0.05678296461701393\n",
            "loss : 0.05388734117150307\n",
            "loss : 0.05798371136188507\n",
            "loss : 0.04737301915884018\n",
            "loss : 0.05551866441965103\n",
            "loss : 0.06294618546962738\n",
            "loss : 0.05205272138118744\n",
            "loss : 0.0470518134534359\n",
            "loss : 0.06199423596262932\n",
            "loss : 0.08147231489419937\n",
            "loss : 0.057322245091199875\n",
            "loss : 0.055759213864803314\n",
            "loss : 0.05915386974811554\n",
            "loss : 0.055843863636255264\n",
            "loss : 0.06608443707227707\n",
            "loss : 0.04881223663687706\n",
            "loss : 0.054770711809396744\n",
            "loss : 0.050498198717832565\n",
            "loss : 0.06375862658023834\n",
            "loss : 0.052179042249917984\n",
            "loss : 0.04337169975042343\n",
            "loss : 0.058522362262010574\n",
            "loss : 0.055551089346408844\n",
            "loss : 0.05538085848093033\n",
            "loss : 0.04959532991051674\n",
            "loss : 0.04828527197241783\n",
            "loss : 0.04130455479025841\n",
            "loss : 0.04545891284942627\n",
            "loss : 0.04722122102975845\n",
            "loss : 0.05669892579317093\n",
            "loss : 0.055709358304739\n",
            "loss : 0.05033162608742714\n",
            "loss : 0.06329721957445145\n",
            "loss : 0.053815096616744995\n",
            "loss : 0.06783182919025421\n",
            "loss : 0.04739800840616226\n",
            "loss : 0.02938517928123474\n",
            "epoch done ! 1  loss :  6.319212161004543\n",
            "loss : 0.06258991360664368\n",
            "loss : 0.05264756456017494\n",
            "loss : 0.05951842665672302\n",
            "loss : 0.04352891817688942\n",
            "loss : 0.05179291218519211\n",
            "loss : 0.0505172498524189\n",
            "loss : 0.054074402898550034\n",
            "loss : 0.03878229483962059\n",
            "loss : 0.06575529277324677\n",
            "loss : 0.06953835487365723\n",
            "loss : 0.070501908659935\n",
            "loss : 0.07006107270717621\n",
            "loss : 0.0542033351957798\n",
            "loss : 0.047507394105196\n",
            "loss : 0.05419422686100006\n",
            "loss : 0.052418991923332214\n",
            "loss : 0.04670560359954834\n",
            "loss : 0.03354518488049507\n",
            "loss : 0.05638466775417328\n",
            "loss : 0.036978743970394135\n",
            "loss : 0.05131933465600014\n",
            "loss : 0.048297882080078125\n",
            "loss : 0.04682719334959984\n",
            "loss : 0.040728870779275894\n",
            "loss : 0.03577163442969322\n",
            "loss : 0.0418093241751194\n",
            "loss : 0.04088260605931282\n",
            "loss : 0.05351638421416283\n",
            "loss : 0.04426572471857071\n",
            "loss : 0.04933563992381096\n",
            "loss : 0.05974320322275162\n",
            "loss : 0.049598075449466705\n",
            "loss : 0.0464819110929966\n",
            "loss : 0.05222175642848015\n",
            "loss : 0.06211893633008003\n",
            "loss : 0.04059838131070137\n",
            "loss : 0.05227135121822357\n",
            "loss : 0.04784892499446869\n",
            "loss : 0.06462114304304123\n",
            "loss : 0.06412973254919052\n",
            "loss : 0.0686124786734581\n",
            "loss : 0.04271703213453293\n",
            "loss : 0.0440344400703907\n",
            "loss : 0.051588334143161774\n",
            "loss : 0.06274746358394623\n",
            "loss : 0.04904833436012268\n",
            "loss : 0.057689130306243896\n",
            "loss : 0.04162130132317543\n",
            "loss : 0.04725631698966026\n",
            "loss : 0.03872273489832878\n",
            "loss : 0.04239574074745178\n",
            "loss : 0.06729065626859665\n",
            "loss : 0.039921995252370834\n",
            "loss : 0.050602737814188004\n",
            "loss : 0.04751250147819519\n",
            "loss : 0.03891681134700775\n",
            "loss : 0.06544724851846695\n",
            "loss : 0.04713841900229454\n",
            "loss : 0.05102972686290741\n",
            "loss : 0.05698560178279877\n",
            "loss : 0.0545634962618351\n",
            "loss : 0.06439170241355896\n",
            "loss : 0.0342596173286438\n",
            "loss : 0.03723834082484245\n",
            "loss : 0.04350320249795914\n",
            "loss : 0.033036984503269196\n",
            "loss : 0.035910964012145996\n",
            "loss : 0.04614350199699402\n",
            "loss : 0.051723308861255646\n",
            "loss : 0.06983537971973419\n",
            "loss : 0.035437826067209244\n",
            "loss : 0.03886963054537773\n",
            "loss : 0.05686367675662041\n",
            "loss : 0.0614277645945549\n",
            "loss : 0.0468791201710701\n",
            "loss : 0.0605439692735672\n",
            "loss : 0.047304265201091766\n",
            "loss : 0.05755073204636574\n",
            "loss : 0.05715581029653549\n",
            "loss : 0.04159948229789734\n",
            "loss : 0.04027204215526581\n",
            "loss : 0.02842055819928646\n",
            "loss : 0.0560433566570282\n",
            "loss : 0.0535740964114666\n",
            "loss : 0.044165436178445816\n",
            "loss : 0.04352487996220589\n",
            "loss : 0.048543550074100494\n",
            "loss : 0.05750440061092377\n",
            "loss : 0.04132559150457382\n",
            "loss : 0.043153271079063416\n",
            "loss : 0.05112138390541077\n",
            "loss : 0.051274094730615616\n",
            "loss : 0.031545866280794144\n",
            "loss : 0.04491928592324257\n",
            "loss : 0.042056262493133545\n",
            "loss : 0.047367122024297714\n",
            "loss : 0.03313790261745453\n",
            "loss : 0.04528508335351944\n",
            "loss : 0.04426675662398338\n",
            "loss : 0.06235840171575546\n",
            "loss : 0.06511784344911575\n",
            "loss : 0.06427092850208282\n",
            "loss : 0.05193064734339714\n",
            "loss : 0.042280714958906174\n",
            "loss : 0.05913941189646721\n",
            "loss : 0.043832164257764816\n",
            "loss : 0.03715614974498749\n",
            "loss : 0.06742048263549805\n",
            "loss : 0.055566392838954926\n",
            "epoch done ! 2  loss :  5.44572632946074\n",
            "loss : 0.036049194633960724\n",
            "loss : 0.052451036870479584\n",
            "loss : 0.044264569878578186\n",
            "loss : 0.029164684936404228\n",
            "loss : 0.04529135674238205\n",
            "loss : 0.05484167858958244\n",
            "loss : 0.03167972341179848\n",
            "loss : 0.03260190039873123\n",
            "loss : 0.04499513655900955\n",
            "loss : 0.04941704124212265\n",
            "loss : 0.048708461225032806\n",
            "loss : 0.040986496955156326\n",
            "loss : 0.04011146351695061\n",
            "loss : 0.03387600928544998\n",
            "loss : 0.04174690693616867\n",
            "loss : 0.04194393381476402\n",
            "loss : 0.03486287221312523\n",
            "loss : 0.04109789803624153\n",
            "loss : 0.031664084643125534\n",
            "loss : 0.03549003601074219\n",
            "loss : 0.050871822983026505\n",
            "loss : 0.04636702686548233\n",
            "loss : 0.054060839116573334\n",
            "loss : 0.04450751841068268\n",
            "loss : 0.026888683438301086\n",
            "loss : 0.07029150426387787\n",
            "loss : 0.06096590682864189\n",
            "loss : 0.03275039792060852\n",
            "loss : 0.044887371361255646\n",
            "loss : 0.029337259009480476\n",
            "loss : 0.04120404273271561\n",
            "loss : 0.06173243373632431\n",
            "loss : 0.05237000063061714\n",
            "loss : 0.030096663162112236\n",
            "loss : 0.044099919497966766\n",
            "loss : 0.05649040639400482\n",
            "loss : 0.05013344809412956\n",
            "loss : 0.04611776024103165\n",
            "loss : 0.03547989949584007\n",
            "loss : 0.03618979454040527\n",
            "loss : 0.04318605363368988\n",
            "loss : 0.04682092368602753\n",
            "loss : 0.03281809389591217\n",
            "loss : 0.03952924907207489\n",
            "loss : 0.03769565746188164\n",
            "loss : 0.02942335233092308\n",
            "loss : 0.04543248564004898\n",
            "loss : 0.040108587592840195\n",
            "loss : 0.030057229101657867\n",
            "loss : 0.029287563636898994\n",
            "loss : 0.04286990314722061\n",
            "loss : 0.0344051830470562\n",
            "loss : 0.04230406507849693\n",
            "loss : 0.035531722009181976\n",
            "loss : 0.05216895043849945\n",
            "loss : 0.0471586175262928\n",
            "loss : 0.045632217079401016\n",
            "loss : 0.04663721099495888\n",
            "loss : 0.03336787223815918\n",
            "loss : 0.026658013463020325\n",
            "loss : 0.03399490565061569\n",
            "loss : 0.04086173698306084\n",
            "loss : 0.030656401067972183\n",
            "loss : 0.03732676059007645\n",
            "loss : 0.03888338804244995\n",
            "loss : 0.040200766175985336\n",
            "loss : 0.038191940635442734\n",
            "loss : 0.03339254856109619\n",
            "loss : 0.05214400961995125\n",
            "loss : 0.03870753198862076\n",
            "loss : 0.028188413009047508\n",
            "loss : 0.034057363867759705\n",
            "loss : 0.03898121044039726\n",
            "loss : 0.03794389218091965\n",
            "loss : 0.03416728973388672\n",
            "loss : 0.04955001547932625\n",
            "loss : 0.03446471691131592\n",
            "loss : 0.03922620788216591\n",
            "loss : 0.032016463577747345\n",
            "loss : 0.051319994032382965\n",
            "loss : 0.028519844636321068\n",
            "loss : 0.02856796234846115\n",
            "loss : 0.044306568801403046\n",
            "loss : 0.036671269685029984\n",
            "loss : 0.030402665957808495\n",
            "loss : 0.041044872254133224\n",
            "loss : 0.04330287128686905\n",
            "loss : 0.029504861682653427\n",
            "loss : 0.04002808406949043\n",
            "loss : 0.032975949347019196\n",
            "loss : 0.03568556532263756\n",
            "loss : 0.048227280378341675\n",
            "loss : 0.03958183154463768\n",
            "loss : 0.02469901740550995\n",
            "loss : 0.04115387052297592\n",
            "loss : 0.036639101803302765\n",
            "loss : 0.03743983805179596\n",
            "loss : 0.03050978109240532\n",
            "loss : 0.04124264046549797\n",
            "loss : 0.03463800996541977\n",
            "loss : 0.031709037721157074\n",
            "loss : 0.0588928647339344\n",
            "loss : 0.038957804441452026\n",
            "loss : 0.05780602619051933\n",
            "loss : 0.04263058304786682\n",
            "loss : 0.03897732123732567\n",
            "loss : 0.02652643620967865\n",
            "loss : 0.03665298968553543\n",
            "loss : 0.03253350034356117\n",
            "epoch done ! 3  loss :  4.361284142360091\n",
            "loss : 0.023246319964528084\n",
            "loss : 0.040357302874326706\n",
            "loss : 0.0344647541642189\n",
            "loss : 0.03899085521697998\n",
            "loss : 0.025015071034431458\n",
            "loss : 0.02177559956908226\n",
            "loss : 0.029718976467847824\n",
            "loss : 0.032622985541820526\n",
            "loss : 0.022874537855386734\n",
            "loss : 0.02539226971566677\n",
            "loss : 0.027125028893351555\n",
            "loss : 0.04468308389186859\n",
            "loss : 0.026687130331993103\n",
            "loss : 0.025982890278100967\n",
            "loss : 0.041320495307445526\n",
            "loss : 0.018087467178702354\n",
            "loss : 0.022875936701893806\n",
            "loss : 0.024291547015309334\n",
            "loss : 0.031042661517858505\n",
            "loss : 0.02976887859404087\n",
            "loss : 0.024048062041401863\n",
            "loss : 0.03200794383883476\n",
            "loss : 0.0400405116379261\n",
            "loss : 0.019689427688717842\n",
            "loss : 0.01878780499100685\n",
            "loss : 0.040252845734357834\n",
            "loss : 0.025178881362080574\n",
            "loss : 0.026002399623394012\n",
            "loss : 0.020677853375673294\n",
            "loss : 0.028741637244820595\n",
            "loss : 0.02490377426147461\n",
            "loss : 0.031111614778637886\n",
            "loss : 0.017205430194735527\n",
            "loss : 0.03125941753387451\n",
            "loss : 0.03618521988391876\n",
            "loss : 0.027895141392946243\n",
            "loss : 0.03175252303481102\n",
            "loss : 0.030267955735325813\n",
            "loss : 0.029654867947101593\n",
            "loss : 0.03808838501572609\n",
            "loss : 0.03708650916814804\n",
            "loss : 0.02851373888552189\n",
            "loss : 0.037973418831825256\n",
            "loss : 0.023574223741889\n",
            "loss : 0.02934015356004238\n",
            "loss : 0.033180516213178635\n",
            "loss : 0.03271417319774628\n",
            "loss : 0.02876555733382702\n",
            "loss : 0.01952342689037323\n",
            "loss : 0.028433537110686302\n",
            "loss : 0.02539757639169693\n",
            "loss : 0.0317009873688221\n",
            "loss : 0.02722364477813244\n",
            "loss : 0.04182467237114906\n",
            "loss : 0.03895756974816322\n",
            "loss : 0.021663730964064598\n",
            "loss : 0.050042103976011276\n",
            "loss : 0.03988243266940117\n",
            "loss : 0.03899224475026131\n",
            "loss : 0.034426942467689514\n",
            "loss : 0.03685111925005913\n",
            "loss : 0.03442692756652832\n",
            "loss : 0.032247621566057205\n",
            "loss : 0.031771447509527206\n",
            "loss : 0.034984201192855835\n",
            "loss : 0.01821276918053627\n",
            "loss : 0.03483140841126442\n",
            "loss : 0.032316066324710846\n",
            "loss : 0.029979895800352097\n",
            "loss : 0.025607649236917496\n",
            "loss : 0.044106874614953995\n",
            "loss : 0.029377875849604607\n",
            "loss : 0.020737865939736366\n",
            "loss : 0.028893059119582176\n",
            "loss : 0.021761396899819374\n",
            "loss : 0.019102521240711212\n",
            "loss : 0.024671897292137146\n",
            "loss : 0.03617748245596886\n",
            "loss : 0.017836617305874825\n",
            "loss : 0.02358798123896122\n",
            "loss : 0.03811449930071831\n",
            "loss : 0.023610251024365425\n",
            "loss : 0.04744972661137581\n",
            "loss : 0.038059670478105545\n",
            "loss : 0.03312766179442406\n",
            "loss : 0.03136424347758293\n",
            "loss : 0.026136960834264755\n",
            "loss : 0.03978238254785538\n",
            "loss : 0.0159396193921566\n",
            "loss : 0.026488211005926132\n",
            "loss : 0.026958655565977097\n",
            "loss : 0.018427260220050812\n",
            "loss : 0.028247596696019173\n",
            "loss : 0.02600765787065029\n",
            "loss : 0.03764890506863594\n",
            "loss : 0.021679628640413284\n",
            "loss : 0.020993757992982864\n",
            "loss : 0.03630158677697182\n",
            "loss : 0.03830084204673767\n",
            "loss : 0.03299912437796593\n",
            "loss : 0.024687696248292923\n",
            "loss : 0.03268055245280266\n",
            "loss : 0.030177103355526924\n",
            "loss : 0.03352189064025879\n",
            "loss : 0.03235330432653427\n",
            "loss : 0.04652005806565285\n",
            "loss : 0.03680805489420891\n",
            "loss : 0.023038644343614578\n",
            "loss : 0.021479958668351173\n",
            "epoch done ! 4  loss :  3.275680832564831\n",
            "loss : 0.018076391890645027\n",
            "loss : 0.03144663944840431\n",
            "loss : 0.010521966964006424\n",
            "loss : 0.018733210861682892\n",
            "loss : 0.023220907896757126\n",
            "loss : 0.025480560958385468\n",
            "loss : 0.027635103091597557\n",
            "loss : 0.02170283906161785\n",
            "loss : 0.02168889530003071\n",
            "loss : 0.03621206805109978\n",
            "loss : 0.015174600295722485\n",
            "loss : 0.012692933902144432\n",
            "loss : 0.018824366852641106\n",
            "loss : 0.02106098085641861\n",
            "loss : 0.03270786628127098\n",
            "loss : 0.020620793104171753\n",
            "loss : 0.017090581357479095\n",
            "loss : 0.021007481962442398\n",
            "loss : 0.012273302301764488\n",
            "loss : 0.028038116171956062\n",
            "loss : 0.020107435062527657\n",
            "loss : 0.02198782190680504\n",
            "loss : 0.01949998177587986\n",
            "loss : 0.017407774925231934\n",
            "loss : 0.015364243648946285\n",
            "loss : 0.01480535976588726\n",
            "loss : 0.02423272840678692\n",
            "loss : 0.017432088032364845\n",
            "loss : 0.027226850390434265\n",
            "loss : 0.017906391993165016\n",
            "loss : 0.022419895976781845\n",
            "loss : 0.019023926928639412\n",
            "loss : 0.023238804191350937\n",
            "loss : 0.023350145667791367\n",
            "loss : 0.030456876382231712\n",
            "loss : 0.01859564706683159\n",
            "loss : 0.010770964436233044\n",
            "loss : 0.03592190518975258\n",
            "loss : 0.025723055005073547\n",
            "loss : 0.03095100447535515\n",
            "loss : 0.03279353678226471\n",
            "loss : 0.029123935848474503\n",
            "loss : 0.020499279722571373\n",
            "loss : 0.020197754725813866\n",
            "loss : 0.015523075126111507\n",
            "loss : 0.02298528328537941\n",
            "loss : 0.021870054304599762\n",
            "loss : 0.023285917937755585\n",
            "loss : 0.02321590483188629\n",
            "loss : 0.008831611834466457\n",
            "loss : 0.02715512551367283\n",
            "loss : 0.032199688255786896\n",
            "loss : 0.03322896733880043\n",
            "loss : 0.024654582142829895\n",
            "loss : 0.02686428651213646\n",
            "loss : 0.025700164958834648\n",
            "loss : 0.013308634981513023\n",
            "loss : 0.021061096340417862\n",
            "loss : 0.0350596085190773\n",
            "loss : 0.02179630473256111\n",
            "loss : 0.022116396576166153\n",
            "loss : 0.017700521275401115\n",
            "loss : 0.014592388644814491\n",
            "loss : 0.014620211906731129\n",
            "loss : 0.018417194485664368\n",
            "loss : 0.02906438708305359\n",
            "loss : 0.014673653058707714\n",
            "loss : 0.015082565136253834\n",
            "loss : 0.029457388445734978\n",
            "loss : 0.02516990713775158\n",
            "loss : 0.025190874934196472\n",
            "loss : 0.02264587953686714\n",
            "loss : 0.020246412605047226\n",
            "loss : 0.020724397152662277\n",
            "loss : 0.018109014257788658\n",
            "loss : 0.03118215687572956\n",
            "loss : 0.028470730409026146\n",
            "loss : 0.027092479169368744\n",
            "loss : 0.012640044093132019\n",
            "loss : 0.023734068498015404\n",
            "loss : 0.011266998015344143\n",
            "loss : 0.03146885707974434\n",
            "loss : 0.019715555012226105\n",
            "loss : 0.021505681797862053\n",
            "loss : 0.022114142775535583\n",
            "loss : 0.024588007479906082\n",
            "loss : 0.026178469881415367\n",
            "loss : 0.009114916436374187\n",
            "loss : 0.01547522284090519\n",
            "loss : 0.015670496970415115\n",
            "loss : 0.020522627979516983\n",
            "loss : 0.02851293608546257\n",
            "loss : 0.024659324437379837\n",
            "loss : 0.03265568986535072\n",
            "loss : 0.015900494530797005\n",
            "loss : 0.02184293046593666\n",
            "loss : 0.010944117791950703\n",
            "loss : 0.012153621762990952\n",
            "loss : 0.027529634535312653\n",
            "loss : 0.027514277026057243\n",
            "loss : 0.01917436346411705\n",
            "loss : 0.01886649988591671\n",
            "loss : 0.03836813196539879\n",
            "loss : 0.03262873366475105\n",
            "loss : 0.031610991805791855\n",
            "loss : 0.018421299755573273\n",
            "loss : 0.018282413482666016\n",
            "loss : 0.020634721964597702\n",
            "loss : 0.021542321890592575\n",
            "epoch done ! 5  loss :  2.419781473465264\n",
            "loss : 0.022877296432852745\n",
            "loss : 0.021195000037550926\n",
            "loss : 0.01758001185953617\n",
            "loss : 0.012111539952456951\n",
            "loss : 0.01232081651687622\n",
            "loss : 0.018767599016427994\n",
            "loss : 0.02629128284752369\n",
            "loss : 0.013055102899670601\n",
            "loss : 0.02343410812318325\n",
            "loss : 0.019656695425510406\n",
            "loss : 0.018653983250260353\n",
            "loss : 0.02061646617949009\n",
            "loss : 0.01913290098309517\n",
            "loss : 0.011314313858747482\n",
            "loss : 0.018783167004585266\n",
            "loss : 0.012422249652445316\n",
            "loss : 0.012725703418254852\n",
            "loss : 0.011272980831563473\n",
            "loss : 0.018910806626081467\n",
            "loss : 0.015538061037659645\n",
            "loss : 0.03349810466170311\n",
            "loss : 0.019842620939016342\n",
            "loss : 0.028741849586367607\n",
            "loss : 0.0219342652708292\n",
            "loss : 0.025318648666143417\n",
            "loss : 0.021698476746678352\n",
            "loss : 0.01610015518963337\n",
            "loss : 0.017993906512856483\n",
            "loss : 0.021836208179593086\n",
            "loss : 0.03836054727435112\n",
            "loss : 0.017693452537059784\n",
            "loss : 0.009128866717219353\n",
            "loss : 0.01472909189760685\n",
            "loss : 0.007652766536921263\n",
            "loss : 0.015416678041219711\n",
            "loss : 0.021751219406723976\n",
            "loss : 0.01607905514538288\n",
            "loss : 0.00976074393838644\n",
            "loss : 0.013422290794551373\n",
            "loss : 0.013401190750300884\n",
            "loss : 0.01526171900331974\n",
            "loss : 0.0266962219029665\n",
            "loss : 0.016086425632238388\n",
            "loss : 0.01981978863477707\n",
            "loss : 0.006143791601061821\n",
            "loss : 0.017028722912073135\n",
            "loss : 0.02738230861723423\n",
            "loss : 0.01770324818789959\n",
            "loss : 0.019263222813606262\n",
            "loss : 0.012387271970510483\n",
            "loss : 0.007682066410779953\n",
            "loss : 0.01078647468239069\n",
            "loss : 0.018632734194397926\n",
            "loss : 0.016228746622800827\n",
            "loss : 0.03575538098812103\n",
            "loss : 0.022179994732141495\n",
            "loss : 0.020040813833475113\n",
            "loss : 0.00895282719284296\n",
            "loss : 0.0067034028470516205\n",
            "loss : 0.015177932567894459\n",
            "loss : 0.009365081787109375\n",
            "loss : 0.01607363112270832\n",
            "loss : 0.02281200885772705\n",
            "loss : 0.016179079189896584\n",
            "loss : 0.025212595239281654\n",
            "loss : 0.009728197008371353\n",
            "loss : 0.022719386965036392\n",
            "loss : 0.02206672914326191\n",
            "loss : 0.01715479977428913\n",
            "loss : 0.024572430178523064\n",
            "loss : 0.0189632847905159\n",
            "loss : 0.014270668849349022\n",
            "loss : 0.014539607800543308\n",
            "loss : 0.01604045182466507\n",
            "loss : 0.01882155053317547\n",
            "loss : 0.010688749141991138\n",
            "loss : 0.008538494817912579\n",
            "loss : 0.015454083681106567\n",
            "loss : 0.01585286855697632\n",
            "loss : 0.009157604537904263\n",
            "loss : 0.015693800523877144\n",
            "loss : 0.010259893722832203\n",
            "loss : 0.007896807044744492\n",
            "loss : 0.01467086747288704\n",
            "loss : 0.008754734881222248\n",
            "loss : 0.027195211499929428\n",
            "loss : 0.0106592932716012\n",
            "loss : 0.020250294357538223\n",
            "loss : 0.01669975183904171\n",
            "loss : 0.005050111562013626\n",
            "loss : 0.013217748142778873\n",
            "loss : 0.01524845790117979\n",
            "loss : 0.014531818218529224\n",
            "loss : 0.015383736230432987\n",
            "loss : 0.01680968701839447\n",
            "loss : 0.012128026224672794\n",
            "loss : 0.021266689524054527\n",
            "loss : 0.008377217687666416\n",
            "loss : 0.023750342428684235\n",
            "loss : 0.02080845646560192\n",
            "loss : 0.015312687493860722\n",
            "loss : 0.007756461855024099\n",
            "loss : 0.008902257308363914\n",
            "loss : 0.019856976345181465\n",
            "loss : 0.021418962627649307\n",
            "loss : 0.018325956538319588\n",
            "loss : 0.009441693313419819\n",
            "loss : 0.018224909901618958\n",
            "loss : 0.02319168858230114\n",
            "epoch done ! 6  loss :  1.840229163877666\n",
            "loss : 0.018728304654359818\n",
            "loss : 0.014181232079863548\n",
            "loss : 0.024233046919107437\n",
            "loss : 0.017374366521835327\n",
            "loss : 0.007765941321849823\n",
            "loss : 0.01383565180003643\n",
            "loss : 0.00545434420928359\n",
            "loss : 0.004797808825969696\n",
            "loss : 0.018549377098679543\n",
            "loss : 0.01362798921763897\n",
            "loss : 0.016567407175898552\n",
            "loss : 0.008879829198122025\n",
            "loss : 0.02834503725171089\n",
            "loss : 0.0129478070884943\n",
            "loss : 0.01201749499887228\n",
            "loss : 0.004962420091032982\n",
            "loss : 0.004306618589907885\n",
            "loss : 0.0040878732688724995\n",
            "loss : 0.014201341196894646\n",
            "loss : 0.011021456681191921\n",
            "loss : 0.021635061129927635\n",
            "loss : 0.006666922476142645\n",
            "loss : 0.02159912884235382\n",
            "loss : 0.03193580359220505\n",
            "loss : 0.004427962005138397\n",
            "loss : 0.009935955516994\n",
            "loss : 0.016883058473467827\n",
            "loss : 0.012913787737488747\n",
            "loss : 0.009829596616327763\n",
            "loss : 0.014299464412033558\n",
            "loss : 0.013696114532649517\n",
            "loss : 0.013321117497980595\n",
            "loss : 0.011561593040823936\n",
            "loss : 0.016334787011146545\n",
            "loss : 0.004471722524613142\n",
            "loss : 0.006193878129124641\n",
            "loss : 0.01978207379579544\n",
            "loss : 0.024046961218118668\n",
            "loss : 0.012505260296165943\n",
            "loss : 0.011360165663063526\n",
            "loss : 0.013998644426465034\n",
            "loss : 0.019947148859500885\n",
            "loss : 0.026758477091789246\n",
            "loss : 0.00421727541834116\n",
            "loss : 0.014287815429270267\n",
            "loss : 0.007767570670694113\n",
            "loss : 0.011980992741882801\n",
            "loss : 0.012941441498696804\n",
            "loss : 0.01137626077979803\n",
            "loss : 0.0157804973423481\n",
            "loss : 0.01427889708429575\n",
            "loss : 0.014722233638167381\n",
            "loss : 0.013861428946256638\n",
            "loss : 0.010196831077337265\n",
            "loss : 0.009849024936556816\n",
            "loss : 0.013586027547717094\n",
            "loss : 0.009914974682033062\n",
            "loss : 0.0014518004609271884\n",
            "loss : 0.014394434168934822\n",
            "loss : 0.008184066042304039\n",
            "loss : 0.008006254211068153\n",
            "loss : 0.00611519068479538\n",
            "loss : 0.018024777993559837\n",
            "loss : 0.019557015970349312\n",
            "loss : 0.009106779471039772\n",
            "loss : 0.014766150154173374\n",
            "loss : 0.018661117181181908\n",
            "loss : 0.020647630095481873\n",
            "loss : 0.016561251133680344\n",
            "loss : 0.019372424110770226\n",
            "loss : 0.0077384947799146175\n",
            "loss : 0.018544746562838554\n",
            "loss : 0.008692754432559013\n",
            "loss : 0.009141835384070873\n",
            "loss : 0.00540393590927124\n",
            "loss : 0.005697946529835463\n",
            "loss : 0.013385934755206108\n",
            "loss : 0.014290384948253632\n",
            "loss : 0.018843531608581543\n",
            "loss : 0.007191022392362356\n",
            "loss : 0.004983780439943075\n",
            "loss : 0.00825448241084814\n",
            "loss : 0.007653975393623114\n",
            "loss : 0.012698120437562466\n",
            "loss : 0.023783762007951736\n",
            "loss : 0.019505521282553673\n",
            "loss : 0.01822926290333271\n",
            "loss : 0.02600596472620964\n",
            "loss : 0.012344935908913612\n",
            "loss : 0.007829979062080383\n",
            "loss : 0.010151194408535957\n",
            "loss : 0.018172388896346092\n",
            "loss : 0.006343410816043615\n",
            "loss : 0.006798412650823593\n",
            "loss : 0.02697078138589859\n",
            "loss : 0.019401943311095238\n",
            "loss : 0.01464898232370615\n",
            "loss : 0.015072518028318882\n",
            "loss : 0.01210637204349041\n",
            "loss : 0.010634652338922024\n",
            "loss : 0.014665556140244007\n",
            "loss : 0.01515477430075407\n",
            "loss : 0.014192252419888973\n",
            "loss : 0.009199632331728935\n",
            "loss : 0.012148686684668064\n",
            "loss : 0.012110326439142227\n",
            "loss : 0.01233639009296894\n",
            "loss : 0.007055573631078005\n",
            "loss : 0.006783502176403999\n",
            "epoch done ! 7  loss :  1.43376791884657\n",
            "loss : 0.004277207888662815\n",
            "loss : 0.011747158132493496\n",
            "loss : 0.009292320348322392\n",
            "loss : 0.008128478191792965\n",
            "loss : 0.013952956534922123\n",
            "loss : 0.006332315970212221\n",
            "loss : 0.012478595599532127\n",
            "loss : 0.014007124118506908\n",
            "loss : 0.011323024518787861\n",
            "loss : 0.011694523505866528\n",
            "loss : 0.018372008576989174\n",
            "loss : 0.014739973470568657\n",
            "loss : 0.014640538953244686\n",
            "loss : 0.011509991250932217\n",
            "loss : 0.012041168287396431\n",
            "loss : 0.013211985118687153\n",
            "loss : 0.009217808954417706\n",
            "loss : 0.015117558650672436\n",
            "loss : 0.016730688512325287\n",
            "loss : 0.004482821095734835\n",
            "loss : 0.0064664618112146854\n",
            "loss : 0.005613869987428188\n",
            "loss : 0.020855577662587166\n",
            "loss : 0.009892378002405167\n",
            "loss : 0.012042107991874218\n",
            "loss : 0.008888806216418743\n",
            "loss : 0.009156564250588417\n",
            "loss : 0.012442117556929588\n",
            "loss : 0.009769193828105927\n",
            "loss : 0.010250583291053772\n",
            "loss : 0.015622404403984547\n",
            "loss : 0.00620855251327157\n",
            "loss : 0.012643581256270409\n",
            "loss : 0.013798579573631287\n",
            "loss : 0.009182361885905266\n",
            "loss : 0.006850262638181448\n",
            "loss : 0.006438208743929863\n",
            "loss : 0.014135858975350857\n",
            "loss : 0.006830243393778801\n",
            "loss : 0.011219603940844536\n",
            "loss : 0.011773786507546902\n",
            "loss : 0.01230740174651146\n",
            "loss : 0.008519000373780727\n",
            "loss : 0.010322640649974346\n",
            "loss : 0.012476473115384579\n",
            "loss : 0.00905908178538084\n",
            "loss : 0.0030767552088946104\n",
            "loss : 0.0045591844245791435\n",
            "loss : 0.01185520738363266\n",
            "loss : 0.0180374663323164\n",
            "loss : 0.002852122765034437\n",
            "loss : 0.0023687747307121754\n",
            "loss : 0.0038595744408667088\n",
            "loss : 0.011255848221480846\n",
            "loss : 0.018574852496385574\n",
            "loss : 0.008479810319840908\n",
            "loss : 0.017309924587607384\n",
            "loss : 0.015327854081988335\n",
            "loss : 0.013123683631420135\n",
            "loss : 0.00839006993919611\n",
            "loss : 0.011657626368105412\n",
            "loss : 0.02404012717306614\n",
            "loss : 0.00896019209176302\n",
            "loss : 0.009578925557434559\n",
            "loss : 0.01396291982382536\n",
            "loss : 0.014335229061543941\n",
            "loss : 0.020534416660666466\n",
            "loss : 0.01074491161853075\n",
            "loss : 0.00939217023551464\n",
            "loss : 0.002539594192057848\n",
            "loss : 0.0052932631224393845\n",
            "loss : 0.010817097499966621\n",
            "loss : 0.009193814359605312\n",
            "loss : 0.004054193384945393\n",
            "loss : 0.012922189198434353\n",
            "loss : 0.01282543782144785\n",
            "loss : 0.0024955456610769033\n",
            "loss : 0.009760607965290546\n",
            "loss : 0.0075996615923941135\n",
            "loss : 0.012230352498590946\n",
            "loss : 0.025048132985830307\n",
            "loss : 0.009585048072040081\n",
            "loss : 0.0040186564438045025\n",
            "loss : 0.005407002288848162\n",
            "loss : 0.01860450580716133\n",
            "loss : 0.006686373148113489\n",
            "loss : 0.010461132042109966\n",
            "loss : 0.015153026208281517\n",
            "loss : 0.01144811138510704\n",
            "loss : 0.008046004921197891\n",
            "loss : 0.00903540849685669\n",
            "loss : 0.013117068447172642\n",
            "loss : 0.013059865683317184\n",
            "loss : 0.009262720122933388\n",
            "loss : 0.0017039034282788634\n",
            "loss : 0.011398636735975742\n",
            "loss : 0.010982614941895008\n",
            "loss : 0.007271457463502884\n",
            "loss : 0.0016185875283554196\n",
            "loss : 0.010925102047622204\n",
            "loss : 0.011941214092075825\n",
            "loss : 0.012762609869241714\n",
            "loss : 0.01111365482211113\n",
            "loss : 0.018385669216513634\n",
            "loss : 0.011052883230149746\n",
            "loss : 0.005234913434833288\n",
            "loss : 0.010690638795495033\n",
            "loss : 0.007446350064128637\n",
            "loss : 0.001136626466177404\n",
            "epoch done ! 8  loss :  1.150671272422187\n",
            "loss : 0.011357026174664497\n",
            "loss : 0.01136046927422285\n",
            "loss : 0.005310563370585442\n",
            "loss : 0.00577819999307394\n",
            "loss : 0.0028143124654889107\n",
            "loss : 0.012434071861207485\n",
            "loss : 0.0073009696789085865\n",
            "loss : 0.004189395345747471\n",
            "loss : 0.008097206242382526\n",
            "loss : 0.007631025277078152\n",
            "loss : 0.003781635779887438\n",
            "loss : 0.012445094995200634\n",
            "loss : 0.007123901974409819\n",
            "loss : 0.0088075902312994\n",
            "loss : 0.0035536957439035177\n",
            "loss : 0.009975858964025974\n",
            "loss : 0.009717874228954315\n",
            "loss : 0.004294995684176683\n",
            "loss : 0.014933853410184383\n",
            "loss : 0.012062035501003265\n",
            "loss : 0.0028166233096271753\n",
            "loss : 0.005119589623063803\n",
            "loss : 0.005755571182817221\n",
            "loss : 0.011662081815302372\n",
            "loss : 0.014927113428711891\n",
            "loss : 0.010649267584085464\n",
            "loss : 0.0017762663774192333\n",
            "loss : 0.006923694163560867\n",
            "loss : 0.005719992332160473\n",
            "loss : 0.015359456650912762\n",
            "loss : 0.010317081585526466\n",
            "loss : 0.005478809121996164\n",
            "loss : 0.004960738588124514\n",
            "loss : 0.01344247255474329\n",
            "loss : 0.004466661252081394\n",
            "loss : 0.004972873255610466\n",
            "loss : 0.0066994111984968185\n",
            "loss : 0.013385758735239506\n",
            "loss : 0.005940604954957962\n",
            "loss : 0.0064440383575856686\n",
            "loss : 0.007715639192610979\n",
            "loss : 0.015399247407913208\n",
            "loss : 0.003505447879433632\n",
            "loss : 0.004539418965578079\n",
            "loss : 0.017368031665682793\n",
            "loss : 0.005844363011419773\n",
            "loss : 0.004013333003968\n",
            "loss : 0.005455331411212683\n",
            "loss : 0.01026289351284504\n",
            "loss : 0.014253784902393818\n",
            "loss : 0.0036345398984849453\n",
            "loss : 0.007813728414475918\n",
            "loss : 0.01039690151810646\n",
            "loss : 0.0024806049186736345\n",
            "loss : 0.003661830211058259\n",
            "loss : 0.013864588923752308\n",
            "loss : 0.006772329099476337\n",
            "loss : 0.010325802490115166\n",
            "loss : 0.007307363674044609\n",
            "loss : 0.0030006254091858864\n",
            "loss : 0.01661880686879158\n",
            "loss : 0.011554297059774399\n",
            "loss : 0.014539144933223724\n",
            "loss : 0.0046083047054708\n",
            "loss : 0.009051377885043621\n",
            "loss : 0.009561640210449696\n",
            "loss : 0.007236428093165159\n",
            "loss : 0.006803816184401512\n",
            "loss : 0.015429598279297352\n",
            "loss : 0.015564278699457645\n",
            "loss : 0.004795657470822334\n",
            "loss : 0.014036879874765873\n",
            "loss : 0.006097074598073959\n",
            "loss : 0.008021930232644081\n",
            "loss : 0.012287245132029057\n",
            "loss : 0.013428456149995327\n",
            "loss : 0.011206397786736488\n",
            "loss : 0.007310105487704277\n",
            "loss : 0.007556980475783348\n",
            "loss : 0.003517714561894536\n",
            "loss : 0.011628856882452965\n",
            "loss : 0.004739176016300917\n",
            "loss : 0.018568582832813263\n",
            "loss : 0.010577214881777763\n",
            "loss : 0.002646883251145482\n",
            "loss : 0.005703849717974663\n",
            "loss : 0.008927376940846443\n",
            "loss : 0.0065942020155489445\n",
            "loss : 0.008128625340759754\n",
            "loss : 0.009811414405703545\n",
            "loss : 0.004695737734436989\n",
            "loss : 0.01289287954568863\n",
            "loss : 0.010909387841820717\n",
            "loss : 0.013902890495955944\n",
            "loss : 0.014445466920733452\n",
            "loss : 0.009017711505293846\n",
            "loss : 0.007184972055256367\n",
            "loss : 0.009815969504415989\n",
            "loss : 0.01167748961597681\n",
            "loss : 0.00848107784986496\n",
            "loss : 0.01524223294109106\n",
            "loss : 0.0102715864777565\n",
            "loss : 0.009532168507575989\n",
            "loss : 0.0034406939521431923\n",
            "loss : 0.018131371587514877\n",
            "loss : 0.009337163530290127\n",
            "loss : 0.007868197746574879\n",
            "loss : 0.01567697525024414\n",
            "loss : 0.0005512660136446357\n",
            "epoch done ! 9  loss :  0.9570312438299879\n",
            "loss : 0.0011228001676499844\n",
            "loss : 0.020266929641366005\n",
            "loss : 0.00792016927152872\n",
            "loss : 0.013015931472182274\n",
            "loss : 0.004461699165403843\n",
            "loss : 0.009097510017454624\n",
            "loss : 0.002973605180159211\n",
            "loss : 0.0009049703949131072\n",
            "loss : 0.008356004022061825\n",
            "loss : 0.010368878953158855\n",
            "loss : 0.012855436652898788\n",
            "loss : 0.01030785869807005\n",
            "loss : 0.013043303973972797\n",
            "loss : 0.009016010910272598\n",
            "loss : 0.007760258391499519\n",
            "loss : 0.012157406657934189\n",
            "loss : 0.0025289382319897413\n",
            "loss : 0.007961694151163101\n",
            "loss : 0.007238070946186781\n",
            "loss : 0.004866098519414663\n",
            "loss : 0.00893435813486576\n",
            "loss : 0.008966105058789253\n",
            "loss : 0.01204126887023449\n",
            "loss : 0.007554888259619474\n",
            "loss : 0.002974680159240961\n",
            "loss : 0.01075813639909029\n",
            "loss : 0.006126037333160639\n",
            "loss : 0.005564173683524132\n",
            "loss : 0.002934712450951338\n",
            "loss : 0.007267531473189592\n",
            "loss : 0.0024191776756197214\n",
            "loss : 0.01794380322098732\n",
            "loss : 0.003275312250480056\n",
            "loss : 0.003529835259541869\n",
            "loss : 0.005981546360999346\n",
            "loss : 0.007443578448146582\n",
            "loss : 0.004642111249268055\n",
            "loss : 0.009118749760091305\n",
            "loss : 0.007754470221698284\n",
            "loss : 0.00998377799987793\n",
            "loss : 0.009154986590147018\n",
            "loss : 0.00944787822663784\n",
            "loss : 0.011567141860723495\n",
            "loss : 0.001504797488451004\n",
            "loss : 0.016650745645165443\n",
            "loss : 0.0066749402321875095\n",
            "loss : 0.008876112289726734\n",
            "loss : 0.00555948494002223\n",
            "loss : 0.010347900912165642\n",
            "loss : 0.004820038098841906\n",
            "loss : 0.005024084821343422\n",
            "loss : 0.011832168325781822\n",
            "loss : 0.011370707303285599\n",
            "loss : 0.009035696275532246\n",
            "loss : 0.008906100876629353\n",
            "loss : 0.004209908656775951\n",
            "loss : 0.00538972020149231\n",
            "loss : 0.0011614118702709675\n",
            "loss : 0.0015943298349156976\n",
            "loss : 0.002386826789006591\n",
            "loss : 0.0026912232860922813\n",
            "loss : 0.004335025791078806\n",
            "loss : 0.005347524769604206\n",
            "loss : 0.0018452582880854607\n",
            "loss : 0.0072367205284535885\n",
            "loss : 0.016856247559189796\n",
            "loss : 0.005507525522261858\n",
            "loss : 0.005281890742480755\n",
            "loss : 0.007288524881005287\n",
            "loss : 0.005789507180452347\n",
            "loss : 0.005518471822142601\n",
            "loss : 0.01346546970307827\n",
            "loss : 0.011809715069830418\n",
            "loss : 0.013186507858335972\n",
            "loss : 0.012766025960445404\n",
            "loss : 0.01280037872493267\n",
            "loss : 0.007718208245933056\n",
            "loss : 0.008484050631523132\n",
            "loss : 0.004843860864639282\n",
            "loss : 0.006748836487531662\n",
            "loss : 0.0037377309054136276\n",
            "loss : 0.006261641625314951\n",
            "loss : 0.0025040805339813232\n",
            "loss : 0.013773185200989246\n",
            "loss : 0.00816718302667141\n",
            "loss : 0.004826392978429794\n",
            "loss : 0.007553972769528627\n",
            "loss : 0.004889102652668953\n",
            "loss : 0.006653185933828354\n",
            "loss : 0.006988600827753544\n",
            "loss : 0.002067754976451397\n",
            "loss : 0.002425477607175708\n",
            "loss : 0.0038116283249109983\n",
            "loss : 0.007471160497516394\n",
            "loss : 0.004997990559786558\n",
            "loss : 0.005188161972910166\n",
            "loss : 0.010797901079058647\n",
            "loss : 0.010441900230944157\n",
            "loss : 0.014031725004315376\n",
            "loss : 0.0038982718251645565\n",
            "loss : 0.003622154938057065\n",
            "loss : 0.005020239390432835\n",
            "loss : 0.005673864856362343\n",
            "loss : 0.005836509633809328\n",
            "loss : 0.0076853325590491295\n",
            "loss : 0.000641603663098067\n",
            "loss : 0.0024865628220140934\n",
            "loss : 0.011484053917229176\n",
            "loss : 0.00688461447134614\n",
            "epoch done ! 10  loss :  0.7962957696290687\n",
            "loss : 0.013880417682230473\n",
            "loss : 0.0019487743265926838\n",
            "loss : 0.0024533045943826437\n",
            "loss : 0.004196370951831341\n",
            "loss : 0.005270525813102722\n",
            "loss : 0.010161466896533966\n",
            "loss : 0.008092602714896202\n",
            "loss : 0.0022233014460653067\n",
            "loss : 0.0022687925957143307\n",
            "loss : 0.005100778304040432\n",
            "loss : 0.00439552403986454\n",
            "loss : 0.004345616791397333\n",
            "loss : 0.0008895036298781633\n",
            "loss : 0.0034125768579542637\n",
            "loss : 0.006141485180705786\n",
            "loss : 0.006822461728006601\n",
            "loss : 0.0024122546892613173\n",
            "loss : 0.004519824404269457\n",
            "loss : 0.007479683496057987\n",
            "loss : 0.0058554611168801785\n",
            "loss : 0.003539162687957287\n",
            "loss : 0.0065361796878278255\n",
            "loss : 0.0040796250104904175\n",
            "loss : 0.012423676438629627\n",
            "loss : 0.0032617244869470596\n",
            "loss : 0.003198327962309122\n",
            "loss : 0.005008811596781015\n",
            "loss : 0.013539006933569908\n",
            "loss : 0.002073099371045828\n",
            "loss : 0.005190885625779629\n",
            "loss : 0.010870402678847313\n",
            "loss : 0.0035173778887838125\n",
            "loss : 0.007827005349099636\n",
            "loss : 0.00870503019541502\n",
            "loss : 0.00932317040860653\n",
            "loss : 0.0005117307882755995\n",
            "loss : 0.010582191869616508\n",
            "loss : 0.009537615813314915\n",
            "loss : 0.006431897170841694\n",
            "loss : 0.013885349035263062\n",
            "loss : 0.006236229557543993\n",
            "loss : 0.0071981605142354965\n",
            "loss : 0.00243840622715652\n",
            "loss : 0.0007695092936046422\n",
            "loss : 0.007464598398655653\n",
            "loss : 0.00889855157583952\n",
            "loss : 0.001998497638851404\n",
            "loss : 0.0045007625594735146\n",
            "loss : 0.005017131567001343\n",
            "loss : 0.004347633104771376\n",
            "loss : 0.0043893600814044476\n",
            "loss : 0.00302793993614614\n",
            "loss : 0.004431113600730896\n",
            "loss : 0.004103409126400948\n",
            "loss : 0.002771131694316864\n",
            "loss : 0.004524995572865009\n",
            "loss : 0.005720979534089565\n",
            "loss : 0.002275309758260846\n",
            "loss : 0.0070103188045322895\n",
            "loss : 0.007023542188107967\n",
            "loss : 0.005977623164653778\n",
            "loss : 0.00942117627710104\n",
            "loss : 0.005669707432389259\n",
            "loss : 0.0044042086228728294\n",
            "loss : 0.004208804108202457\n",
            "loss : 0.003327653743326664\n",
            "loss : 0.00921220239251852\n",
            "loss : 0.005423912312835455\n",
            "loss : 0.004583720117807388\n",
            "loss : 0.01092464104294777\n",
            "loss : 0.007849905639886856\n",
            "loss : 0.004859677981585264\n",
            "loss : 0.006704118102788925\n",
            "loss : 0.010558522306382656\n",
            "loss : 0.001897673006169498\n",
            "loss : 0.006539837457239628\n",
            "loss : 0.004577931482344866\n",
            "loss : 0.010010922327637672\n",
            "loss : 0.0005247756489552557\n",
            "loss : 0.016884801909327507\n",
            "loss : 0.006922340020537376\n",
            "loss : 0.0056845019571483135\n",
            "loss : 0.009224536828696728\n",
            "loss : 0.006802637130022049\n",
            "loss : 0.0027927651535719633\n",
            "loss : 0.007865343242883682\n",
            "loss : 0.002814512001350522\n",
            "loss : 0.008264606818556786\n",
            "loss : 0.00893798191100359\n",
            "loss : 0.00539034977555275\n",
            "loss : 0.013484859839081764\n",
            "loss : 0.007846619002521038\n",
            "loss : 0.006832343526184559\n",
            "loss : 0.0024049237836152315\n",
            "loss : 0.012400923296809196\n",
            "loss : 0.006758060306310654\n",
            "loss : 0.004408903885632753\n",
            "loss : 0.009298490360379219\n",
            "loss : 0.005255838390439749\n",
            "loss : 0.0049925073981285095\n",
            "loss : 0.010929138399660587\n",
            "loss : 0.009631990455091\n",
            "loss : 0.006337902508676052\n",
            "loss : 0.003559856442734599\n",
            "loss : 0.0019320957362651825\n",
            "loss : 0.0016365268966183066\n",
            "loss : 0.010048015043139458\n",
            "loss : 0.0014265048084780574\n",
            "loss : 0.002441190183162689\n",
            "epoch done ! 11  loss :  0.6559486591722816\n",
            "loss : 0.005507349967956543\n",
            "loss : 0.006964065600186586\n",
            "loss : 0.003989826887845993\n",
            "loss : 0.005025593098253012\n",
            "loss : 0.002593198325484991\n",
            "loss : 0.0038489047437906265\n",
            "loss : 0.007311014458537102\n",
            "loss : 0.00911763310432434\n",
            "loss : 0.005344298202544451\n",
            "loss : 0.005783271510154009\n",
            "loss : 0.003995305858552456\n",
            "loss : 0.005646229721605778\n",
            "loss : 0.008475743234157562\n",
            "loss : 0.015010634437203407\n",
            "loss : 0.0036383396945893764\n",
            "loss : 0.009084650315344334\n",
            "loss : 0.013299202546477318\n",
            "loss : 0.002169033046811819\n",
            "loss : 0.0018183270003646612\n",
            "loss : 0.0035810505505651236\n",
            "loss : 0.0036341615486890078\n",
            "loss : 0.002777537563815713\n",
            "loss : 0.004844939336180687\n",
            "loss : 0.001687818905338645\n",
            "loss : 0.007164187263697386\n",
            "loss : 0.005965963006019592\n",
            "loss : 0.001898414921015501\n",
            "loss : 0.0066542718559503555\n",
            "loss : 0.004608321934938431\n",
            "loss : 0.004721858073025942\n",
            "loss : 0.003373553045094013\n",
            "loss : 0.003101247362792492\n",
            "loss : 0.003964514937251806\n",
            "loss : 0.0036447809543460608\n",
            "loss : 0.006559088360518217\n",
            "loss : 0.0027190959081053734\n",
            "loss : 0.004072882700711489\n",
            "loss : 0.008157932199537754\n",
            "loss : 0.00241005583666265\n",
            "loss : 0.004352345131337643\n",
            "loss : 0.0072737811133265495\n",
            "loss : 0.0034058918245136738\n",
            "loss : 0.0023178455885499716\n",
            "loss : 0.0017436352791264653\n",
            "loss : 0.003927804064005613\n",
            "loss : 0.001294760382734239\n",
            "loss : 0.0005059497198089957\n",
            "loss : 0.006806229241192341\n",
            "loss : 0.004456372931599617\n",
            "loss : 0.014831088483333588\n",
            "loss : 0.0014720390317961574\n",
            "loss : 0.0029580402188003063\n",
            "loss : 0.004961036145687103\n",
            "loss : 0.010035942308604717\n",
            "loss : 0.0008446457795798779\n",
            "loss : 0.0028104919474571943\n",
            "loss : 0.0030713959131389856\n",
            "loss : 0.005773067008703947\n",
            "loss : 0.008193140849471092\n",
            "loss : 0.00761815020814538\n",
            "loss : 0.001239141565747559\n",
            "loss : 0.007039697840809822\n",
            "loss : 0.0026179293636232615\n",
            "loss : 0.0026301811449229717\n",
            "loss : 0.0022491002455353737\n",
            "loss : 0.0024221756029874086\n",
            "loss : 0.006850864738225937\n",
            "loss : 0.00028859981102868915\n",
            "loss : 0.0066501968540251255\n",
            "loss : 0.003916003741323948\n",
            "loss : 0.0023018433712422848\n",
            "loss : 0.0008106533787213266\n",
            "loss : 0.0029695704579353333\n",
            "loss : 0.002280463697388768\n",
            "loss : 0.0044478438794612885\n",
            "loss : 0.013014600612223148\n",
            "loss : 0.004614006262272596\n",
            "loss : 0.002914919052273035\n",
            "loss : 0.004850876517593861\n",
            "loss : 0.009045545011758804\n",
            "loss : 0.0013646755833178759\n",
            "loss : 0.004862829111516476\n",
            "loss : 0.004058544524013996\n",
            "loss : 0.0017028743168339133\n",
            "loss : 0.0032789455726742744\n",
            "loss : 0.0115007683634758\n",
            "loss : 0.004216571804136038\n",
            "loss : 0.0082930326461792\n",
            "loss : 0.005472412798553705\n",
            "loss : 0.003147848416119814\n",
            "loss : 0.00426406878978014\n",
            "loss : 0.005676240660250187\n",
            "loss : 0.006528833881020546\n",
            "loss : 0.00255904090590775\n",
            "loss : 0.004799467511475086\n",
            "loss : 0.0032953659538179636\n",
            "loss : 0.005753181409090757\n",
            "loss : 0.006939012557268143\n",
            "loss : 0.02326146885752678\n",
            "loss : 0.006968409288674593\n",
            "loss : 0.006473499815911055\n",
            "loss : 0.00430276757106185\n",
            "loss : 0.004528169985860586\n",
            "loss : 0.0029813521541655064\n",
            "loss : 0.006375964730978012\n",
            "loss : 0.0034311164636164904\n",
            "loss : 0.00454803416505456\n",
            "loss : 0.003891417058184743\n",
            "loss : 0.0050932737067341805\n",
            "epoch done ! 12  loss :  0.5455413529416546\n",
            "loss : 0.008647637441754341\n",
            "loss : 0.004073599819093943\n",
            "loss : 0.0036212829872965813\n",
            "loss : 0.0022421679459512234\n",
            "loss : 0.004294481594115496\n",
            "loss : 0.006126524414867163\n",
            "loss : 0.004293771460652351\n",
            "loss : 0.0020585341844707727\n",
            "loss : 0.00308231427334249\n",
            "loss : 0.0008318005711771548\n",
            "loss : 0.0019619420636445284\n",
            "loss : 0.003433093661442399\n",
            "loss : 0.00519662955775857\n",
            "loss : 0.0029832031577825546\n",
            "loss : 0.002498683286830783\n",
            "loss : 0.004546559881418943\n",
            "loss : 0.0025515707675367594\n",
            "loss : 0.004203690681606531\n",
            "loss : 0.0005957219400443137\n",
            "loss : 0.002819602843374014\n",
            "loss : 0.007253467570990324\n",
            "loss : 0.004912352189421654\n",
            "loss : 0.005469552241265774\n",
            "loss : 0.003029404440894723\n",
            "loss : 0.004205749370157719\n",
            "loss : 0.005659676156938076\n",
            "loss : 0.009514037519693375\n",
            "loss : 0.0017581966239959002\n",
            "loss : 0.0030643281061202288\n",
            "loss : 0.002396540716290474\n",
            "loss : 0.00325460615567863\n",
            "loss : 0.008199878968298435\n",
            "loss : 0.0027123019099235535\n",
            "loss : 0.004568738862872124\n",
            "loss : 0.0059105828404426575\n",
            "loss : 0.011151161976158619\n",
            "loss : 0.0060766031965613365\n",
            "loss : 0.0034393572714179754\n",
            "loss : 0.004905483219772577\n",
            "loss : 0.01297069527208805\n",
            "loss : 0.010520567186176777\n",
            "loss : 0.0018732169410213828\n",
            "loss : 0.0033420592080801725\n",
            "loss : 0.011289654299616814\n",
            "loss : 0.005801959428936243\n",
            "loss : 0.0024885262828320265\n",
            "loss : 0.009421600960195065\n",
            "loss : 0.006163845304399729\n",
            "loss : 0.01042361743748188\n",
            "loss : 0.004273793660104275\n",
            "loss : 0.003940248396247625\n",
            "loss : 0.006354823242872953\n",
            "loss : 0.004176836460828781\n",
            "loss : 0.007203050423413515\n",
            "loss : 0.008471111766994\n",
            "loss : 0.005864594131708145\n",
            "loss : 0.0012414543889462948\n",
            "loss : 0.0037768110632896423\n",
            "loss : 0.0023040517698973417\n",
            "loss : 0.0009324525017291307\n",
            "loss : 0.006259506568312645\n",
            "loss : 0.004754623863846064\n",
            "loss : 0.004514947067946196\n",
            "loss : 0.0026367814280092716\n",
            "loss : 0.00036542833549901843\n",
            "loss : 0.001371659804135561\n",
            "loss : 0.0014686411013826728\n",
            "loss : 0.0005517998361028731\n",
            "loss : 0.004000999964773655\n",
            "loss : 0.0014899587258696556\n",
            "loss : 0.005337269511073828\n",
            "loss : 0.002058787504211068\n",
            "loss : 0.0015910498332232237\n",
            "loss : 0.005272442940622568\n",
            "loss : 0.007638684008270502\n",
            "loss : 0.005383679643273354\n",
            "loss : 0.003750244155526161\n",
            "loss : 0.0037374510429799557\n",
            "loss : 0.002540070563554764\n",
            "loss : 0.0015841765562072396\n",
            "loss : 0.009554051794111729\n",
            "loss : 0.003152793040499091\n",
            "loss : 0.004309386480599642\n",
            "loss : 0.002548406133428216\n",
            "loss : 0.005865795072168112\n",
            "loss : 0.007869381457567215\n",
            "loss : 0.004589463118463755\n",
            "loss : 0.0017964189173653722\n",
            "loss : 0.002920887665823102\n",
            "loss : 0.005577544216066599\n",
            "loss : 0.010736213997006416\n",
            "loss : 0.006934382487088442\n",
            "loss : 0.005818091332912445\n",
            "loss : 0.003466247348114848\n",
            "loss : 0.0003133654536213726\n",
            "loss : 0.004679776728153229\n",
            "loss : 0.0035583293065428734\n",
            "loss : 0.004025596659630537\n",
            "loss : 0.004942042753100395\n",
            "loss : 0.001482127234339714\n",
            "loss : 0.0016658988315612078\n",
            "loss : 0.004364146385341883\n",
            "loss : 0.0009169360273517668\n",
            "loss : 0.00276567111723125\n",
            "loss : 0.001058506197296083\n",
            "loss : 0.003228290006518364\n",
            "loss : 0.0013318293495103717\n",
            "loss : 0.0012194974115118384\n",
            "loss : 0.0017449241131544113\n",
            "epoch done ! 13  loss :  0.46912400508881547\n",
            "loss : 0.0024124281480908394\n",
            "loss : 0.0016409470699727535\n",
            "loss : 0.003700347850099206\n",
            "loss : 0.002527811797335744\n",
            "loss : 0.0031217695213854313\n",
            "loss : 0.002619680482894182\n",
            "loss : 0.00043495811405591667\n",
            "loss : 0.003186634974554181\n",
            "loss : 0.006854929495602846\n",
            "loss : 0.0021029808558523655\n",
            "loss : 0.003163320245221257\n",
            "loss : 0.0031790423672646284\n",
            "loss : 0.0025444345083087683\n",
            "loss : 0.003723613917827606\n",
            "loss : 0.0028645461425185204\n",
            "loss : 0.00346434791572392\n",
            "loss : 0.0014211424859240651\n",
            "loss : 0.0009539209422655404\n",
            "loss : 0.0016534255119040608\n",
            "loss : 0.008573014289140701\n",
            "loss : 0.003567501436918974\n",
            "loss : 0.005851727444678545\n",
            "loss : 0.004261250142008066\n",
            "loss : 0.004060845356434584\n",
            "loss : 0.002482231240719557\n",
            "loss : 0.002726556034758687\n",
            "loss : 0.005128218326717615\n",
            "loss : 0.00223045377060771\n",
            "loss : 0.0014666293282061815\n",
            "loss : 0.002508428879082203\n",
            "loss : 0.0050602443516254425\n",
            "loss : 0.00464214151725173\n",
            "loss : 0.0018901695730164647\n",
            "loss : 0.0019493940053507686\n",
            "loss : 0.00030156696448102593\n",
            "loss : 0.0009776920778676867\n",
            "loss : 0.00024265477259177715\n",
            "loss : 0.0014328387333080173\n",
            "loss : 0.0025452824775129557\n",
            "loss : 0.005749561823904514\n",
            "loss : 0.0020975687075406313\n",
            "loss : 0.0013408279046416283\n",
            "loss : 0.0059936195611953735\n",
            "loss : 0.008840322494506836\n",
            "loss : 0.0016399597516283393\n",
            "loss : 0.0076087950728833675\n",
            "loss : 0.002743458840996027\n",
            "loss : 0.006417091935873032\n",
            "loss : 0.007560993544757366\n",
            "loss : 0.004137749318033457\n",
            "loss : 0.006219991482794285\n",
            "loss : 0.0025063962675631046\n",
            "loss : 0.006417900789529085\n",
            "loss : 0.011103913187980652\n",
            "loss : 0.004275938495993614\n",
            "loss : 0.003032974898815155\n",
            "loss : 0.0037778494879603386\n",
            "loss : 0.004596661310642958\n",
            "loss : 0.004673507530242205\n",
            "loss : 0.002393526490777731\n",
            "loss : 0.006516439840197563\n",
            "loss : 0.004745578393340111\n",
            "loss : 0.005249178968369961\n",
            "loss : 0.003301351796835661\n",
            "loss : 0.008253522217273712\n",
            "loss : 0.0035848019178956747\n",
            "loss : 0.0010623763082548976\n",
            "loss : 0.0025209691375494003\n",
            "loss : 0.004254644736647606\n",
            "loss : 0.0060639576986432076\n",
            "loss : 0.0011587823973968625\n",
            "loss : 0.0002518458350095898\n",
            "loss : 0.004628017544746399\n",
            "loss : 0.004643631167709827\n",
            "loss : 0.00392652302980423\n",
            "loss : 0.003191011492162943\n",
            "loss : 0.0041733053512871265\n",
            "loss : 0.003982114139944315\n",
            "loss : 0.004557821899652481\n",
            "loss : 0.00810251384973526\n",
            "loss : 0.005448601208627224\n",
            "loss : 0.0012321813264861703\n",
            "loss : 0.003260234137997031\n",
            "loss : 0.0069557977840304375\n",
            "loss : 0.0025629375595599413\n",
            "loss : 0.005929330829530954\n",
            "loss : 0.002685178304091096\n",
            "loss : 0.0006565913208760321\n",
            "loss : 0.010110966861248016\n",
            "loss : 0.0018970377277582884\n",
            "loss : 0.0009526455542072654\n",
            "loss : 0.005558736156672239\n",
            "loss : 0.00456336559727788\n",
            "loss : 0.0029726114589720964\n",
            "loss : 0.0027197450399398804\n",
            "loss : 0.0005994216189719737\n",
            "loss : 0.003317561699077487\n",
            "loss : 0.003557293675839901\n",
            "loss : 0.0016961318906396627\n",
            "loss : 0.004599515814334154\n",
            "loss : 0.004937204532325268\n",
            "loss : 0.006564249284565449\n",
            "loss : 0.0007104389951564372\n",
            "loss : 0.0001208197936648503\n",
            "loss : 0.0010093020973727107\n",
            "loss : 0.0024218293838202953\n",
            "loss : 0.0012271509040147066\n",
            "loss : 0.0030815291684120893\n",
            "loss : 0.00774729810655117\n",
            "epoch done ! 14  loss :  0.39986182752181776\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8A0oIOBPre8e",
        "colab_type": "code",
        "outputId": "c3fbae44-2ce4-453e-919f-0da9ff4eed68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "predictions=[]\n",
        "gt=[]\n",
        "\n",
        "for sentences,labels,mask in train_loader:\n",
        "    sentences=sentences.cuda()\n",
        "    mask=mask.cuda()\n",
        "    preds=net(sentences,mask)\n",
        "    #print(preds.shape)\n",
        "    preds=F.sigmoid(preds)\n",
        "    #print(preds.shape)\n",
        "    \n",
        "    labels_=labels.cpu().detach().numpy().flatten()\n",
        "    gt.extend(labels_)\n",
        "    \n",
        "    preds=preds.cpu().detach().numpy().flatten()\n",
        "    \n",
        "    for p in preds:\n",
        "        \n",
        "        if p>0.2:\n",
        "            predictions.append(1)\n",
        "        else:\n",
        "            predictions.append(0)\n",
        "\n",
        "gt=np.array(gt)\n",
        "predictions = np.array(predictions)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "c=classification_report(gt,predictions)\n",
        "\n",
        "print(c.split('\\n'))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['              precision    recall  f1-score   support', '', '         0.0       1.00      1.00      1.00    140286', '         1.0       0.97      0.99      0.98      2312', '', '    accuracy                           1.00    142598', '   macro avg       0.98      1.00      0.99    142598', 'weighted avg       1.00      1.00      1.00    142598', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4zq4tE8rnmD",
        "colab_type": "code",
        "outputId": "28dd4712-342c-4085-8a47-ac03c78b1d7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "c.split('\\n')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['              precision    recall  f1-score   support',\n",
              " '',\n",
              " '         0.0       1.00      1.00      1.00    140286',\n",
              " '         1.0       0.97      0.99      0.98      2312',\n",
              " '',\n",
              " '    accuracy                           1.00    142598',\n",
              " '   macro avg       0.98      1.00      0.99    142598',\n",
              " 'weighted avg       1.00      1.00      1.00    142598',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8ZUBE90rsFD",
        "colab_type": "code",
        "outputId": "a640c64a-68c4-40a6-f38c-afcbb4a35b08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "all_labels=[]\n",
        "test_data=np.load('test_data.npy',allow_pickle=True)\n",
        "for row in test_data:\n",
        "    if row[-1]=='':\n",
        "        continue\n",
        "    else:\n",
        "        all_labels.extend(row[-1]['category'])\n",
        "\n",
        "all_labels=list(set(all_labels))\n",
        "print(all_labels)\n",
        "print(\"number of classes = \",len(all_labels))\n",
        "\n",
        "x_test=[]\n",
        "y_test=[]\n",
        "for row in test_data:\n",
        "    sentence = row[2]\n",
        "    \n",
        "    x_test.append(sentence)\n",
        "    \n",
        "    if row[-1]=='':\n",
        "        y_test.append([0])\n",
        "        continue\n",
        "    a=[]\n",
        "    labels = row[-1]['category']\n",
        "    for i in labels:\n",
        "        if i not in labels_mapping.keys():\n",
        "          a.append(0)\n",
        "        else:\n",
        "          a.append(labels_mapping[i])\n",
        "    y_test.append(a)\n",
        "\n",
        "tokenized_sentences = []\n",
        "labels=[]\n",
        "\n",
        "for (s,l) in zip(x_test,y_test) :\n",
        "    if isinstance(s,float):\n",
        "        continue\n",
        "    b=tokenizer.encode(s,add_sepcial_tokens=True)\n",
        "    tokenized_sentences.append(b)\n",
        "    labels.append(l)\n",
        "\n",
        "max_len = 0\n",
        "for i in tokenized_sentences:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized_sentences])\n",
        "input_ids = torch.LongTensor(np.array(padded))\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "class TestLoader(data.Dataset):\n",
        "\n",
        "  def __init__(self,sentences,new_y_test,y_test,attention_mask):\n",
        "\n",
        "    self.sentences=sentences\n",
        "    self.new_y_train=new_y_train\n",
        "    self.attention_mask=attention_mask\n",
        "    self.y_test=y_test\n",
        "    self.length=len(self.sentences)\n",
        "\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "    image=self.sentences[idx]\n",
        "    label=self.new_y_train[idx]\n",
        "    mask = self.attention_mask[idx]\n",
        "    original =self.y_test[idx]\n",
        "    return image,label,mask,original\n",
        "  \n",
        "  def __len__(self):\n",
        "\n",
        "    return self.length\n",
        "\n",
        "\n",
        "new_y_test=[]\n",
        "#https://discuss.pytorch.org/t/what-kind-of-loss-is-better-to-use-in-multilabel-classification/32203/4\n",
        "\n",
        "for label in y_test:\n",
        "    #print(label)\n",
        "    a=torch.LongTensor(label)\n",
        "    a=a.unsqueeze(0)\n",
        "    #print(a)\n",
        "    target = torch.zeros(a.size(0),82).scatter_(1, a, 1.)\n",
        "    new_y_test.append(target.numpy()[0])\n",
        "\n",
        "new_y_train=torch.FloatTensor(np.array(new_y_test))\n",
        "print(len(input_ids))\n",
        "print(\"y_test = \",y_test[0])\n",
        "\n",
        "test_target=TestLoader(input_ids,new_y_test,y_test,attention_mask)\n",
        "\n",
        "test_loader=torch.utils.data.DataLoader(test_target,batch_size=16,shuffle=True)\n",
        "\n",
        "predictions=[]\n",
        "gt=[]\n",
        "o_labels=[]\n",
        "o_pred=[]\n",
        "index=0\n",
        "for sentences,labels,mask,original in test_loader:\n",
        "    sentences=sentences.cuda()\n",
        "    mask=mask.cuda()\n",
        "    preds=net(sentences,mask)\n",
        "    #print(preds.shape)\n",
        "    preds=F.sigmoid(preds)\n",
        "    #print(preds.shape)\n",
        "    \n",
        "    labels_=labels.cpu().detach().numpy().flatten()\n",
        "    gt.extend(labels_)\n",
        "  \n",
        "    preds=preds.cpu().detach().numpy().flatten()\n",
        "    original_pred=[]\n",
        "    original_label=[]\n",
        "    prediction=[]\n",
        "    for p in preds:\n",
        "        \n",
        "        if p>0.2:\n",
        "            prediction.append(1)\n",
        "        else:\n",
        "            prediction.append(0)\n",
        "    predictions.extend(prediction)\n",
        "    idx =0\n",
        "    for one_label in labels:\n",
        "      for k1 in range(len(one_label)):\n",
        "        if one_label[k1]==0:\n",
        "          original_label.append(0)\n",
        "        else:\n",
        "          original_label.append(k1)\n",
        "        #print(prediction[k1])\n",
        "        if prediction[idx]==0:\n",
        "          original_pred.append(0)\n",
        "        else:\n",
        "          original_pred.append(k1)\n",
        "        idx+=1\n",
        "      o_labels.extend(original_label)\n",
        "      o_pred.extend(original_pred)\n",
        "\n",
        "    index+=1\n",
        "  \n",
        "gt=np.array(gt)\n",
        "predictions = np.array(predictions)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "c1=classification_report(gt,predictions)\n",
        "\n",
        "gt=np.array(o_labels)\n",
        "predictions = np.array(o_pred)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "c2=classification_report(gt,predictions)\n",
        "print(c2.split('\\n'))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['MEMORY#DESIGN_FEATURES', 'LAPTOP#PRICE', 'CPU#OPERATION_PERFORMANCE', 'KEYBOARD#USABILITY', 'MOUSE#USABILITY', 'HARD_DISC#QUALITY', 'GRAPHICS#GENERAL', 'HARDWARE#GENERAL', 'DISPLAY#USABILITY', 'HARDWARE#OPERATION_PERFORMANCE', 'DISPLAY#GENERAL', 'LAPTOP#MISCELLANEOUS', 'LAPTOP#QUALITY', 'LAPTOP#USABILITY', 'MOUSE#OPERATION_PERFORMANCE', 'KEYBOARD#DESIGN_FEATURES', 'DISPLAY#QUALITY', 'SHIPPING#QUALITY', 'FANS_COOLING#QUALITY', 'LAPTOP#DESIGN_FEATURES', 'MOUSE#GENERAL', 'COMPANY#GENERAL', 'SUPPORT#QUALITY', 'BATTERY#QUALITY', 'HARDWARE#QUALITY', 'MULTIMEDIA_DEVICES#GENERAL', 'BATTERY#OPERATION_PERFORMANCE', 'OS#USABILITY', 'DISPLAY#OPERATION_PERFORMANCE', 'BATTERY#DESIGN_FEATURES', 'LAPTOP#OPERATION_PERFORMANCE', 'KEYBOARD#OPERATION_PERFORMANCE', 'LAPTOP#GENERAL', 'LAPTOP#CONNECTIVITY', 'BATTERY#GENERAL', 'OS#OPERATION_PERFORMANCE', 'MULTIMEDIA_DEVICES#USABILITY', 'GRAPHICS#QUALITY', 'OPTICAL_DRIVES#OPERATION_PERFORMANCE', 'SUPPORT#GENERAL', 'DISPLAY#DESIGN_FEATURES', 'SOFTWARE#GENERAL', 'MULTIMEDIA_DEVICES#MISCELLANEOUS', 'KEYBOARD#GENERAL', 'BATTERY#MISCELLANEOUS', 'OPTICAL_DRIVES#QUALITY', 'MOUSE#DESIGN_FEATURES', 'POWER_SUPPLY#QUALITY', 'SOFTWARE#OPERATION_PERFORMANCE', 'KEYBOARD#QUALITY', 'HARD_DISC#DESIGN_FEATURES', 'MOUSE#QUALITY', 'LAPTOP#PORTABILITY', 'SOFTWARE#MISCELLANEOUS', 'SOFTWARE#QUALITY', 'MULTIMEDIA_DEVICES#QUALITY', 'OS#GENERAL', 'CPU#DESIGN_FEATURES']\n",
            "number of classes =  58\n",
            "761\n",
            "y_test =  [44, 57]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1569: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['              precision    recall  f1-score   support', '', '           0       0.99      1.00      0.99    519887', '           1       1.00      1.00      1.00         7', '           2       1.00      0.21      0.35       118', '           3       0.94      0.81      0.87       521', '           4       0.00      0.00      0.00        17', '           5       0.62      0.23      0.34        77', '           6       0.67      0.73      0.70        95', '           7       0.38      0.70      0.49        33', '          11       0.00      0.00      0.00        11', '          15       0.64      0.56      0.60       108', '          16       0.78      0.36      0.49       611', '          17       0.65      0.53      0.58       608', '          19       0.52      0.45      0.48       368', '          21       0.52      0.53      0.53        87', '          23       0.55      0.52      0.54        82', '          24       0.63      0.74      0.68       192', '          25       0.00      0.00      0.00         1', '          28       0.00      0.00      0.00         0', '          29       0.00      0.00      0.00        17', '          30       0.74      0.56      0.64       748', '          31       0.00      0.00      0.00         3', '          32       0.57      0.19      0.28       140', '          33       0.14      0.50      0.21        28', '          36       0.00      0.00      0.00         5', '          37       1.00      0.41      0.58        27', '          38       0.62      0.91      0.74       121', '          39       0.58      0.32      0.42        77', '          40       0.15      0.28      0.20        32', '          44       0.72      0.73      0.72       950', '          45       0.00      0.00      0.00         8', '          46       0.00      0.00      0.00         0', '          47       0.73      0.69      0.71      1766', '          48       0.71      0.50      0.58       246', '          49       0.00      0.00      0.00        23', '          50       0.00      0.00      0.00        35', '          51       0.00      0.00      0.00         1', '          52       0.00      0.00      0.00         1', '          54       0.00      0.00      0.00        13', '          55       0.40      0.58      0.47        62', '          56       0.00      0.00      0.00        48', '          57       0.35      0.37      0.36        73', '          59       0.00      0.00      0.00         1', '          60       0.14      0.11      0.12        38', '          61       0.00      0.00      0.00         5', '          62       0.00      0.00      0.00        16', '          64       0.61      0.62      0.61        65', '          65       1.00      1.00      1.00         7', '          68       0.48      0.86      0.62        14', '          70       0.19      0.35      0.25        20', '          71       0.00      0.00      0.00        19', '          73       1.00      0.42      0.60        33', '          75       0.43      0.58      0.50        91', '          77       0.00      0.00      0.00        10', '          79       0.00      0.00      0.00         9', '          80       0.59      0.25      0.35        68', '          81       0.78      0.62      0.69       191', '', '    accuracy                           0.99    527834', '   macro avg       0.39      0.34      0.34    527834', 'weighted avg       0.99      0.99      0.99    527834', '']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPL36ZeFtFgd",
        "colab_type": "code",
        "outputId": "98e12df7-f7ba-4d16-9d92-10a131977e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "c1.split(\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['              precision    recall  f1-score   support',\n",
              " '',\n",
              " '         0.0       0.99      0.99      0.99     61337',\n",
              " '         1.0       0.65      0.57      0.61      1065',\n",
              " '',\n",
              " '    accuracy                           0.99     62402',\n",
              " '   macro avg       0.82      0.78      0.80     62402',\n",
              " 'weighted avg       0.99      0.99      0.99     62402',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEdNPG-Xun3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), \"last_min.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3la0pmZSeFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "37bbb6dc-3b29-4a4b-d8e5-9553f8528541"
      },
      "source": [
        "labels_mapping"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'BATTERY#GENERAL': 49,\n",
              " 'BATTERY#MISCELLANEOUS': 61,\n",
              " 'BATTERY#OPERATION_PERFORMANCE': 38,\n",
              " 'BATTERY#QUALITY': 31,\n",
              " 'COMPANY#GENERAL': 32,\n",
              " 'CPU#MISCELLANEOUS': 78,\n",
              " 'CPU#OPERATION_PERFORMANCE': 2,\n",
              " 'CPU#QUALITY': 10,\n",
              " 'DISPLAY#DESIGN_FEATURES': 55,\n",
              " 'DISPLAY#GENERAL': 15,\n",
              " 'DISPLAY#MISCELLANEOUS': 67,\n",
              " 'DISPLAY#OPERATION_PERFORMANCE': 40,\n",
              " 'DISPLAY#QUALITY': 24,\n",
              " 'DISPLAY#USABILITY': 11,\n",
              " 'FANS_COOLING#DESIGN_FEATURES': 58,\n",
              " 'FANS_COOLING#OPERATION_PERFORMANCE': 22,\n",
              " 'FANS_COOLING#QUALITY': 29,\n",
              " 'GRAPHICS#DESIGN_FEATURES': 34,\n",
              " 'GRAPHICS#GENERAL': 1,\n",
              " 'GRAPHICS#MISCELLANEOUS': 63,\n",
              " 'GRAPHICS#QUALITY': 51,\n",
              " 'HARDWARE#QUALITY': 36,\n",
              " 'HARD_DISC#DESIGN_FEATURES': 71,\n",
              " 'HARD_DISC#QUALITY': 4,\n",
              " 'KEYBOARD#DESIGN_FEATURES': 23,\n",
              " 'KEYBOARD#GENERAL': 60,\n",
              " 'KEYBOARD#OPERATION_PERFORMANCE': 45,\n",
              " 'KEYBOARD#QUALITY': 70,\n",
              " 'KEYBOARD#USABILITY': 5,\n",
              " 'LAPTOP#CONNECTIVITY': 48,\n",
              " 'LAPTOP#DESIGN_FEATURES': 30,\n",
              " 'LAPTOP#GENERAL': 47,\n",
              " 'LAPTOP#MISCELLANEOUS': 16,\n",
              " 'LAPTOP#OPERATION_PERFORMANCE': 44,\n",
              " 'LAPTOP#PORTABILITY': 75,\n",
              " 'LAPTOP#PRICE': 3,\n",
              " 'LAPTOP#QUALITY': 17,\n",
              " 'LAPTOP#USABILITY': 19,\n",
              " 'MEMORY#DESIGN_FEATURES': 7,\n",
              " 'MOTHERBOARD#QUALITY': 28,\n",
              " 'MOUSE#DESIGN_FEATURES': 64,\n",
              " 'MOUSE#GENERAL': 56,\n",
              " 'MOUSE#OPERATION_PERFORMANCE': 21,\n",
              " 'MOUSE#QUALITY': 73,\n",
              " 'MOUSE#USABILITY': 6,\n",
              " 'MULTIMEDIA_DEVICES#DESIGN_FEATURES': 41,\n",
              " 'MULTIMEDIA_DEVICES#GENERAL': 37,\n",
              " 'MULTIMEDIA_DEVICES#MISCELLANEOUS': 59,\n",
              " 'MULTIMEDIA_DEVICES#OPERATION_PERFORMANCE': 42,\n",
              " 'MULTIMEDIA_DEVICES#QUALITY': 80,\n",
              " 'MULTIMEDIA_DEVICES#USABILITY': 52,\n",
              " 'NIL': 0,\n",
              " 'OPTICAL_DRIVES#QUALITY': 62,\n",
              " 'OS#DESIGN_FEATURES': 35,\n",
              " 'OS#GENERAL': 81,\n",
              " 'OS#MISCELLANEOUS': 53,\n",
              " 'OS#OPERATION_PERFORMANCE': 50,\n",
              " 'OS#QUALITY': 26,\n",
              " 'OS#USABILITY': 39,\n",
              " 'PORTS#DESIGN_FEATURES': 72,\n",
              " 'PORTS#OPERATION_PERFORMANCE': 76,\n",
              " 'PORTS#QUALITY': 20,\n",
              " 'POWER_SUPPLY#DESIGN_FEATURES': 69,\n",
              " 'POWER_SUPPLY#MISCELLANEOUS': 8,\n",
              " 'POWER_SUPPLY#OPERATION_PERFORMANCE': 14,\n",
              " 'POWER_SUPPLY#QUALITY': 65,\n",
              " 'SHIPPING#PRICE': 12,\n",
              " 'SHIPPING#QUALITY': 25,\n",
              " 'SOFTWARE#DESIGN_FEATURES': 27,\n",
              " 'SOFTWARE#GENERAL': 57,\n",
              " 'SOFTWARE#MISCELLANEOUS': 77,\n",
              " 'SOFTWARE#OPERATION_PERFORMANCE': 68,\n",
              " 'SOFTWARE#PRICE': 74,\n",
              " 'SOFTWARE#QUALITY': 79,\n",
              " 'SOFTWARE#USABILITY': 46,\n",
              " 'SUPPORT#GENERAL': 54,\n",
              " 'SUPPORT#MISCELLANEOUS': 18,\n",
              " 'SUPPORT#PRICE': 66,\n",
              " 'SUPPORT#QUALITY': 33,\n",
              " 'WARRANTY#GENERAL': 13,\n",
              " 'WARRANTY#MISCELLANEOUS': 43,\n",
              " 'WARRANTY#PRICE': 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tARtMxoK3MKX",
        "colab_type": "code",
        "outputId": "76873921-783b-4718-c57a-12f2d288ae58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "c2.split('\\n')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['              precision    recall  f1-score   support',\n",
              " '',\n",
              " '           0       0.99      1.00      0.99    519887',\n",
              " '           1       1.00      1.00      1.00         7',\n",
              " '           2       1.00      0.21      0.35       118',\n",
              " '           3       0.94      0.81      0.87       521',\n",
              " '           4       0.00      0.00      0.00        17',\n",
              " '           5       0.62      0.23      0.34        77',\n",
              " '           6       0.67      0.73      0.70        95',\n",
              " '           7       0.38      0.70      0.49        33',\n",
              " '          11       0.00      0.00      0.00        11',\n",
              " '          15       0.64      0.56      0.60       108',\n",
              " '          16       0.78      0.36      0.49       611',\n",
              " '          17       0.65      0.53      0.58       608',\n",
              " '          19       0.52      0.45      0.48       368',\n",
              " '          21       0.52      0.53      0.53        87',\n",
              " '          23       0.55      0.52      0.54        82',\n",
              " '          24       0.63      0.74      0.68       192',\n",
              " '          25       0.00      0.00      0.00         1',\n",
              " '          28       0.00      0.00      0.00         0',\n",
              " '          29       0.00      0.00      0.00        17',\n",
              " '          30       0.74      0.56      0.64       748',\n",
              " '          31       0.00      0.00      0.00         3',\n",
              " '          32       0.57      0.19      0.28       140',\n",
              " '          33       0.14      0.50      0.21        28',\n",
              " '          36       0.00      0.00      0.00         5',\n",
              " '          37       1.00      0.41      0.58        27',\n",
              " '          38       0.62      0.91      0.74       121',\n",
              " '          39       0.58      0.32      0.42        77',\n",
              " '          40       0.15      0.28      0.20        32',\n",
              " '          44       0.72      0.73      0.72       950',\n",
              " '          45       0.00      0.00      0.00         8',\n",
              " '          46       0.00      0.00      0.00         0',\n",
              " '          47       0.73      0.69      0.71      1766',\n",
              " '          48       0.71      0.50      0.58       246',\n",
              " '          49       0.00      0.00      0.00        23',\n",
              " '          50       0.00      0.00      0.00        35',\n",
              " '          51       0.00      0.00      0.00         1',\n",
              " '          52       0.00      0.00      0.00         1',\n",
              " '          54       0.00      0.00      0.00        13',\n",
              " '          55       0.40      0.58      0.47        62',\n",
              " '          56       0.00      0.00      0.00        48',\n",
              " '          57       0.35      0.37      0.36        73',\n",
              " '          59       0.00      0.00      0.00         1',\n",
              " '          60       0.14      0.11      0.12        38',\n",
              " '          61       0.00      0.00      0.00         5',\n",
              " '          62       0.00      0.00      0.00        16',\n",
              " '          64       0.61      0.62      0.61        65',\n",
              " '          65       1.00      1.00      1.00         7',\n",
              " '          68       0.48      0.86      0.62        14',\n",
              " '          70       0.19      0.35      0.25        20',\n",
              " '          71       0.00      0.00      0.00        19',\n",
              " '          73       1.00      0.42      0.60        33',\n",
              " '          75       0.43      0.58      0.50        91',\n",
              " '          77       0.00      0.00      0.00        10',\n",
              " '          79       0.00      0.00      0.00         9',\n",
              " '          80       0.59      0.25      0.35        68',\n",
              " '          81       0.78      0.62      0.69       191',\n",
              " '',\n",
              " '    accuracy                           0.99    527834',\n",
              " '   macro avg       0.39      0.34      0.34    527834',\n",
              " 'weighted avg       0.99      0.99      0.99    527834',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    }
  ]
}